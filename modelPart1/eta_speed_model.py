"""
eta_speed_model.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŒ…å«ï¼šNodeAEmbedder, StaticEmbedder, GroupEmbedder,
     NearAggregator, SpeedPredictor
"""
import math

import torch
import torch.nn as nn


class NewsEmbedder(nn.Module):
    """
    é¢„ç•™ï¼šæ–°é—»æ–‡æœ¬ / å¤šæ¨¡æ€æ–°é—»ç‰¹å¾çš„åµŒå…¥å™¨ã€‚
    ç›®å‰ä»…å ä½ï¼Œforward ç›´æ¥æŠ›å‡º NotImplementedErrorã€‚
    ä½ å¯ä»¥åœ¨æ­¤å¤„æ¥å…¥ä»»æ„ BERT / CLIP / LLM + Pooling ç­‰ã€‚
    """

    def __init__(self, d_in: int = 768, d_out: int = 128):
        super().__init__()
        # >>> åœ¨è¿™é‡Œå®ç°ä½ çš„æŠ•å°„ / pooling å±‚ <<<
        raise NotImplementedError("è¯·åœ¨ NewsEmbedder å†…éƒ¨å®ç°å…·ä½“çš„åµŒå…¥é€»è¾‘")

    def forward(self, news_feat: torch.Tensor) -> torch.Tensor:
        """
        news_feat : (B, nB, M, d_in)
          B   â€”â€” batch
          nB  â€”â€” æ¯ä¸ªå‚è€ƒç‚¹ B å¯¹åº”çš„æ–°é—»æ¡æ•°ï¼ˆå¯å˜ï¼Œå·²åœ¨ collate ä¸­ padï¼‰
          M   â€”â€” å•æ¡æ–°é—»çš„ token / patch / frame ç»´åº¦ï¼ˆå¯å˜ï¼‰
        è¿”å› : (B, nB, d_out)
        """
        raise NotImplementedError


class FuseBlock(nn.Module):
    """ raw-128 ä¸ proj-128 â†’ 128  """

    def __init__(self, d=128):
        super().__init__()
        self.fuse = nn.Sequential(
            nn.Linear(d * 2, d * 2),
            nn.GELU(),
            nn.Linear(d * 2, d))

    def forward(self, x_raw, x_proj):  # (...,d) Ã— 2
        return self.fuse(torch.cat([x_raw, x_proj], dim=-1))


# ------------------------- ä½ç½®ç¼–ç  -------------------------
class LearnablePosEncoding(nn.Module):
    """
    å­¦ä¹ å¼ä½ç½®ç¼–ç ï¼Œæ¯”å›ºå®š Sine-Cos æ›´çµæ´»ã€‚
    max_len å–ä¸€ä¸ªå¤Ÿå¤§çš„ä¸Šç•Œï¼ˆ> æ•°æ®ä¸­æœ€é•¿åºåˆ—ï¼‰ï¼Œä¼šè‡ªåŠ¨æˆªæ–­ã€‚
    """

    def __init__(self, d_model: int, max_len: int = 1024):
        super().__init__()
        self.pe = nn.Parameter(torch.zeros(max_len, d_model))  # (T_max, d_model)
        nn.init.normal_(self.pe, mean=0.0, std=0.02)

    def forward(self, x: torch.Tensor, valid_len: torch.LongTensor):
        """
        x         : (B, T, d_model) â€”â€” çº¿æ€§æ˜ å°„åçš„ç‰¹å¾
        valid_len : (B,)            â€”â€” æ¯æ¡æœ‰æ•ˆæ­¥é•¿
        è¿”å›åŒ shape xï¼Œå·²åŠ ä½ç½®ç¼–ç ï¼ˆå¯¹è¶…å‡ºæœ‰æ•ˆé•¿åº¦çš„ padding æ­¥ï¼Œä¸åŠ ï¼‰ã€‚
        """
        B, T, _ = x.shape
        pe = self.pe[:T].unsqueeze(0)  # (1,T,d_model)
        # padding æ­¥ä¸åŠ ä½ç½®ä¿¡æ¯ â†’ mask = (idx < len)
        idx = torch.arange(T, device=x.device).view(1, -1)  # (1,T)
        mask = (idx < valid_len.view(-1, 1)).float()  # (B,T)
        return x + pe * mask.unsqueeze(-1)  # (B,T,d_model)


# ------------------------- NodeAEmbedder -------------------------
class NodeAEmbedder(nn.Module):
    """
    ğŸš¢ è½¨è¿¹åºåˆ— â†’ å‘é‡ (d_model)ã€‚
    é‡‡ç”¨ 1Ã—Linear + å¯å­¦ä¹ ä½ç½®ç¼–ç  + TransformerEncoder + æœ‰æ•ˆæ­¥é•¿å¹³å‡æ± åŒ–ã€‚
    """

    def __init__(self,
                 d_in: int = 7,
                 d_model: int = 64,
                 n_head: int = 8,
                 n_layer: int = 2,
                 ff_mult: int = 4,
                 dropout: float = 0.1,
                 max_len: int = 1024):
        super().__init__()
        self.proj = nn.Linear(d_in, d_model)
        self.pos = LearnablePosEncoding(d_model, max_len)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_head,
            dim_feedforward=d_model * ff_mult,
            dropout=dropout,
            batch_first=True,
            activation="gelu",
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layer)
        self.d_model = d_model

    def forward(self, seq: torch.Tensor, lengths: torch.Tensor):
        """
        å‚æ•°
        ----
        seq     : (N, T_max, 7)          â€”â€” å³ä¾§ 0-padding
        lengths : (N,)                   â€”â€” æœ‰æ•ˆæ­¥é•¿
        è¿”å›
        ----
        out     : (N, d_model)
        """
        x = self.proj(seq)  # (N,T,d_model)
        x = self.pos(x, lengths)  # åŠ ä½ç½®ç¼–ç 
        # key_padding_mask: Padding=true â†’ ä¸å‚ä¸æ³¨æ„åŠ›
        mask = torch.arange(x.size(1), device=x.device)[None, :] >= lengths[:, None]
        z = self.encoder(x, src_key_padding_mask=mask)  # (N,T,d_model)
        # æœ‰æ•ˆæ­¥é•¿å¹³å‡æ± åŒ–
        lens = lengths.clamp(min=1).view(-1, 1)  # é¿å…é™¤ 0
        out = (z * (~mask).unsqueeze(-1)).sum(1) / lens  # (N,d_model)
        return out


# ------------------------- StaticEmbedder -------------------------
class StaticEmbedder(nn.Module):
    """
    16-ç»´èˆªæ¬¡é™æ€ç‰¹å¾ â†’ d_modelã€‚ è¿™é‡Œç”¨ä¸¤å±‚ MLPï¼›ä½ å¯ä»¥æŒ‰éœ€æ”¹æˆæ›´å¤æ‚ç»“æ„ã€‚
    """

    def __init__(self, d_in: int = 16, d_emb: int = 64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(d_in, d_emb),
            nn.GELU(),
            nn.Linear(d_emb, d_emb)
        )

    def forward(self, x: torch.Tensor):
        return self.mlp(x)  # (N,d_emb)


class GroupEmbedder(nn.Module):
    """
    (seq_raw , seq_proj , len , stat) â†’ (N , 128)
    """

    def __init__(self,
                 d_seq_in: int = 7,
                 d_stat_in: int = 16):
        super().__init__()
        d_model = 64
        self.enc = NodeAEmbedder(d_in=d_seq_in, d_model=d_model)
        self.fuse = FuseBlock(d=d_model)
        self.stat = StaticEmbedder(d_in=d_stat_in, d_emb=d_model)

    def forward(self,
                seq_raw: torch.Tensor,  # (N,T,7)
                seq_proj: torch.Tensor,  # (N,T,7)
                lengths: torch.Tensor,  # (N,)
                stat: torch.Tensor):  # (N,16)
        h_raw = self.enc(seq_raw, lengths)
        h_proj = self.enc(seq_proj, lengths)
        h_traj = self.fuse(h_raw, h_proj)  # (N,64)
        h_stat = self.stat(stat)  # (N,64)
        return torch.cat([h_traj, h_stat], dim=-1)  # (N,128)


class NearAggregator(nn.Module):
    """
    æ–¹å‘-è·ç¦»æ³¨æ„åŠ›èšåˆï¼šç»™å®šé‚»èˆ¹åµŒå…¥ + Î”xy + Î”cosÎ¸,sinÎ¸ ä¸ B_query (64),
    äº§ç”ŸåŠ æƒæ± åŒ–çš„è¿‘èˆ¹è¡¨ç¤º (128).
    """

    def __init__(self, d_emb=128, d_q=64):
        super().__init__()
        self.key = nn.Linear(d_emb + 4, d_q)
        self.scale = math.sqrt(d_q)

    def forward(self, near_emb, delta_xy, delta_cs, B_query):
        # near_emb: (B,K,128), delta_xy:(B,K,2), delta_cs:(B,K,2), B_query:(B,64)
        Kcat = torch.cat([near_emb, delta_xy, delta_cs], dim=-1)  # (B,K,132)
        Kproj = self.key(Kcat)  # (B,K,64)
        # ç‚¹ç§¯ / scale -> softmax
        scores = (Kproj * B_query.unsqueeze(1)).sum(-1) / self.scale
        wts = torch.softmax(scores, dim=1)  # (B,K)
        return (wts.unsqueeze(-1) * near_emb).sum(1)  # (B,128)


class SpeedPredictor(nn.Module):
    """
    æœ€ç»ˆé¢„æµ‹ä¸‹ä¸€æ®µé€Ÿåº¦ (km/h)ï¼Œè¾“å…¥ï¼š
      â€¢ B6          : (B,6)
      â€¢ A_emb       : (B,128)
      â€¢ near_emb    : (B,K,128)
      â€¢ delta_xy, delta_cs
      â€¢ ship_emb    : (B,H,128)
    """

    def __init__(self, d_emb=128):
        super().__init__()
        self.near_aggr = NearAggregator(d_emb, 64)
        self.B_proj = nn.Linear(6, 64)
        self.mlp = nn.Sequential(
            nn.Linear(d_emb * 3 + 64, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, B6, A_emb, near_emb, delta_xy, delta_cs, ship_emb):
        # ship_emb: (B,H,128)
        Bq = self.B_proj(B6)  # (B,64)
        nearP = self.near_aggr(near_emb, delta_xy, delta_cs, Bq)  # (B,128)
        shipP = ship_emb.mean(1)  # (B,128)
        fuse = torch.cat([A_emb, nearP, shipP, Bq], dim=-1)  # (B,128*3+64)
        return self.mlp(fuse)  # (B,1)
