{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import folium\n",
    "from datetime import datetime\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def parse_timestamp(ts_str):\n",
    "    \"\"\"解析时间字符串为 datetime 对象。如果不包含时区，则假设为 UTC 并补充 '+00:00'。\"\"\"\n",
    "    if ts_str is None:\n",
    "        return None\n",
    "    # 如果没有时区信息，自动补充\n",
    "    if not (ts_str.endswith(\"Z\") or ('+' in ts_str[-6:] or '-' in ts_str[-6:])):\n",
    "        ts_str += \"+00:00\"\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts_str)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return datetime.strptime(ts_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def get_color_for_error(error, min_error, max_error):\n",
    "    \"\"\"\n",
    "    根据误差值计算颜色。定义自定义颜色序列：红、橙、黄、绿、青、蓝、紫，\n",
    "    且 0 对应绿，负数越大（负值较小）颜色越偏红，正数越大颜色越偏紫。\n",
    "    \"\"\"\n",
    "    # 自定义颜色序列\n",
    "    custom_colors = ['red', 'orange', 'yellow', 'green', 'cyan', 'blue', 'purple']\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"custom_rainbow\", custom_colors)\n",
    "\n",
    "    # 使用对称归一化：以 0 为中心\n",
    "    max_abs = max(abs(min_error), abs(max_error))\n",
    "    if max_abs == 0:\n",
    "        norm_value = 0.5\n",
    "    else:\n",
    "        norm_value = 0.5 + 0.5 * (error / max_abs)\n",
    "        norm_value = max(0, min(1, norm_value))\n",
    "    rgba = custom_cmap(norm_value)\n",
    "    hex_color = mcolors.rgb2hex(rgba)\n",
    "    return hex_color\n",
    "\n",
    "# 文件路径：转换后的 JSON 文件（内容为 JSON 数组）\n",
    "data_path = \"0_4.jsonl\"\n",
    "\n",
    "# 读取 JSON 文件内容\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    voyages = json.load(f)\n",
    "\n",
    "if not isinstance(voyages, list) or len(voyages) == 0:\n",
    "    raise ValueError(\"文件中没有找到有效的航班数据。\")\n",
    "\n",
    "# 为所有航班计算总体中心位置\n",
    "all_lats = []\n",
    "all_lons = []\n",
    "for voyage in voyages:\n",
    "    for pt in voyage.get(\"Path\", []):\n",
    "        if \"latitude\" in pt and \"longitude\" in pt:\n",
    "            all_lats.append(pt[\"latitude\"])\n",
    "            all_lons.append(pt[\"longitude\"])\n",
    "if all_lats and all_lons:\n",
    "    center = [sum(all_lats)/len(all_lats), sum(all_lons)/len(all_lons)]\n",
    "else:\n",
    "    center = [0, 0]\n",
    "\n",
    "# 创建基础地图\n",
    "m = folium.Map(location=center, zoom_start=4)\n",
    "\n",
    "# 定义一组颜色用于航迹折线（每个航班一个颜色）\n",
    "line_colors = [\"blue\", \"green\", \"orange\", \"darkred\", \"cadetblue\",\n",
    "               \"darkgreen\", \"lightblue\", \"pink\", \"gray\", \"black\"]\n",
    "\n",
    "# 遍历所有航班绘制航迹与路径点\n",
    "for idx, voyage in enumerate(voyages):\n",
    "    if idx > 50:\n",
    "        break\n",
    "    path_points = voyage.get(\"Path\", [])\n",
    "    # 提取有效坐标\n",
    "    coords = [(pt[\"latitude\"], pt[\"longitude\"]) for pt in path_points\n",
    "              if \"latitude\" in pt and \"longitude\" in pt]\n",
    "    if not coords:\n",
    "        continue\n",
    "\n",
    "    line_color = line_colors[idx % len(line_colors)]\n",
    "    folium.PolyLine(coords, color=line_color, weight=2.5, opacity=1,\n",
    "                    popup=f\"航班 {idx+1}\").add_to(m)\n",
    "    folium.Marker(coords[0], popup=f\"航班 {idx+1} 起点\", icon=folium.Icon(color=line_color)).add_to(m)\n",
    "    folium.Marker(coords[-1], popup=f\"航班 {idx+1} 终点\", icon=folium.Icon(color=line_color)).add_to(m)\n",
    "\n",
    "    # 计算当前航班最后一个路径点的时间戳\n",
    "    if \"timestamp\" in path_points[-1]:\n",
    "        final_ts = parse_timestamp(path_points[-1][\"timestamp\"])\n",
    "    else:\n",
    "        final_ts = None\n",
    "\n",
    "    # 预先计算所有点的误差 (eta - final_ts)，单位：小时\n",
    "    errors = []\n",
    "    for pt in path_points:\n",
    "        if \"eta\" in pt and final_ts is not None:\n",
    "            eta_ts = parse_timestamp(pt.get(\"eta\"))\n",
    "            if eta_ts is not None:\n",
    "                err = (eta_ts - final_ts).total_seconds() / 3600.0\n",
    "                errors.append(err)\n",
    "    if errors:\n",
    "        min_error = min(errors)\n",
    "        max_error = max(errors)\n",
    "    else:\n",
    "        min_error, max_error = -1, 1\n",
    "\n",
    "    # 对每个路径点添加圆形标记，显示详细信息，并根据误差动态设定颜色\n",
    "    for pt in path_points:\n",
    "        if \"latitude\" in pt and \"longitude\" in pt:\n",
    "            lat = pt[\"latitude\"]\n",
    "            lon = pt[\"longitude\"]\n",
    "            timestamp = pt.get(\"timestamp\", \"N/A\")\n",
    "            speed = pt.get(\"speed\", \"N/A\")\n",
    "            eta = pt.get(\"eta\", \"N/A\")\n",
    "            error_val = None\n",
    "            if \"eta\" in pt and final_ts is not None:\n",
    "                eta_ts = parse_timestamp(pt.get(\"eta\"))\n",
    "                if eta_ts:\n",
    "                    error_val = (eta_ts - final_ts).total_seconds() / 3600.0\n",
    "            if error_val is None:\n",
    "                error_val = 0.0\n",
    "            error_text = f\"{error_val:.2f} 小时\"\n",
    "            marker_color = get_color_for_error(error_val, min_error, max_error)\n",
    "            popup_text = (f\"时间: {timestamp}\\n\"\n",
    "                          f\"速度: {speed}\\n\"\n",
    "                          f\"ETA: {eta}\\n\"\n",
    "                          f\"误差(ETA-最后节点): {error_text}\")\n",
    "            folium.CircleMarker(\n",
    "                location=[lat, lon],\n",
    "                radius=4,\n",
    "                color=marker_color,\n",
    "                fill=True,\n",
    "                fill_color=marker_color,\n",
    "                popup=popup_text\n",
    "            ).add_to(m)\n",
    "output_html = \"voyage_path.html\"\n",
    "m.save(output_html)\n",
    "print(f\"地图已保存为 {output_html}\")\n",
    "m\n"
   ],
   "id": "1609a4005c250c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# —— 配置 —— #\n",
    "data_dir    = \"\"   # 数据目录\n",
    "pattern     = os.path.join(data_dir, \"*.jsonl\")\n",
    "target_mmsi = 205784000                              # 要查询的船 MMSI\n",
    "\n",
    "# —— 日志设置 —— #\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(\"voyage_counter\")\n",
    "\n",
    "# —— 统计变量 —— #\n",
    "total_count = 0\n",
    "files = sorted(glob.glob(pattern))\n",
    "if not files:\n",
    "    logger.error(\"未在目录 %s 下找到任何 .jsonl 文件。\", data_dir)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "logger.info(\"开始遍历 %d 个文件，目标 MMSI=%s\", len(files), target_mmsi)\n",
    "\n",
    "for file_path in files:\n",
    "    logger.info(\"正在处理文件：%s\", os.path.basename(file_path))\n",
    "    per_file_count = 0\n",
    "\n",
    "    # 尝试将整个文件当 JSON 数组解析\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            voyages = json.load(f)\n",
    "        logger.debug(\"  成功以 JSON 数组格式加载，共 %d 条记录。\", len(voyages))\n",
    "    except json.JSONDecodeError:\n",
    "        # 回退到逐行解析 jsonl\n",
    "        logger.warning(\"  文件不是纯 JSON 数组，回退到逐行解析。\")\n",
    "        voyages = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for lineno, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    voyages.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.error(\"    第 %d 行 JSON 解析失败，内容：%s\", lineno, line[:50])\n",
    "        logger.info(\"  逐行解析完成，共收集 %d 条记录。\", len(voyages))\n",
    "\n",
    "    # 验证记录格式、并统计目标 MMSI 出现次数\n",
    "    for idx, voyage in enumerate(voyages, start=1):\n",
    "        mmsi = voyage.get(\"MMSI\")\n",
    "        if mmsi is None:\n",
    "            logger.warning(\"    第 %d 条记录缺少 MMSI 字段，跳过。\", idx)\n",
    "            continue\n",
    "        if mmsi == target_mmsi:\n",
    "            per_file_count += 1\n",
    "\n",
    "    logger.info(\"  在该文件中出现了 %d 次。\", per_file_count)\n",
    "    total_count += per_file_count\n",
    "\n",
    "# —— 输出最终结果 —— #\n",
    "logger.info(\"统计完成。MMSI=%s 的船在所有文件中共出现 %d 次航程记录。\",\n",
    "            target_mmsi, total_count)\n",
    "\n"
   ],
   "id": "394ed76019451688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# —— 配置 —— #\n",
    "data_dir    = \"\"   # 数据目录\n",
    "pattern     = os.path.join(data_dir, \"*.jsonl\")\n",
    "target_port = \"Puerto Quetzal\"                        # 要查询的港口名称\n",
    "\n",
    "# —— 日志设置 —— #\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(\"port_counter\")\n",
    "\n",
    "# —— 统计变量 —— #\n",
    "total_count = 0\n",
    "files = sorted(glob.glob(pattern))\n",
    "if not files:\n",
    "    logger.error(\"未在目录 %s 下找到任何 .jsonl 文件。\", data_dir)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "logger.info(\"开始遍历 %d 个文件，目标港口：%s\", len(files), target_port)\n",
    "\n",
    "for file_path in files:\n",
    "    logger.info(\"处理文件：%s\", os.path.basename(file_path))\n",
    "    per_file_count = 0\n",
    "\n",
    "    # 先尝试将整个文件当作 JSON 数组加载\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            voyages = json.load(f)\n",
    "        logger.debug(\"  以 JSON 数组格式加载，共 %d 条记录。\", len(voyages))\n",
    "    except json.JSONDecodeError:\n",
    "        # 回退到逐行解析\n",
    "        logger.warning(\"  文件不是纯 JSON 数组，回退到逐行解析 jsonl。\")\n",
    "        voyages = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for lineno, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    voyages.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.error(\"    第 %d 行解析失败，内容前50字符：%s\", lineno, line[:50])\n",
    "        logger.info(\"  逐行解析完成，共 %d 条记录。\", len(voyages))\n",
    "\n",
    "    # 遍历记录，检查 Start Port 或 End Port\n",
    "    for idx, voyage in enumerate(voyages, start=1):\n",
    "        # 检查 Start Port\n",
    "        sp = voyage.get(\"Start Port\")\n",
    "        if isinstance(sp, dict):\n",
    "            name = sp.get(\"Main Port Name\")\n",
    "            if name == target_port:\n",
    "                per_file_count += 1\n",
    "        else:\n",
    "            logger.debug(\"    记录 %d 的 Start Port 字段缺失或格式不对。\", idx)\n",
    "\n",
    "        # 检查 End Port\n",
    "        ep = voyage.get(\"End Port\")\n",
    "        if isinstance(ep, dict):\n",
    "            name = ep.get(\"Main Port Name\")\n",
    "            if name == target_port:\n",
    "                per_file_count += 1\n",
    "        else:\n",
    "            logger.debug(\"    记录 %d 的 End Port 字段缺失或格式不对。\", idx)\n",
    "\n",
    "    logger.info(\"  在该文件中共匹配到 %d 次。\", per_file_count)\n",
    "    total_count += per_file_count\n",
    "\n",
    "logger.info(\"统计完成。港口 [%s] 共出现 %d 次。\", target_port, total_count)\n"
   ],
   "id": "40f07b7cb4ed7228"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "import_and_split_fresh.py\n",
    "\n",
    "功能：\n",
    " 1. 连接到 eta_voyage1 数据库\n",
    " 2. DROP 旧表 voyage_node, voyage_main（如果有）\n",
    " 3. CREATE EXTENSION postgis\n",
    " 4. 新建 voyage_main, voyage_node（含 train 列）\n",
    " 5. 遍历当前目录下所有 *.jsonl 文件，批量插入 voyage_main 和 voyage_node\n",
    "    • voyage_main 插入时随机 5% 标为 valid=False，其余 train=True\n",
    "    • voyage_node 继承对应 voyage_main 的 train 标记\n",
    "    • 插入后更新 geom\n",
    " 6. tqdm 显示每个文件导入的进度和记录数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== 配置 ==========\n",
    "DB_PARAMS = {\n",
    "    'dbname':   'eta_voyage1',\n",
    "    'user':     'postgres',\n",
    "    'password': 'y1=x2-c30',\n",
    "    'host':     'localhost',\n",
    "    'port':     5432\n",
    "}\n",
    "DATA_DIR    = '..'  # 与脚本同目录下的 *.jsonl\n",
    "MAIN_BATCH  = 200        # 主表每批插入大小\n",
    "NODE_BATCH  = 5000       # 节点表每批插入大小\n",
    "VALID_RATIO = 0.05       # 5% 划为验证集 (train=False)\n",
    "\n",
    "# ========== DDL ==========\n",
    "\n",
    "DDL_DROP = \"\"\"\n",
    "DROP TABLE IF EXISTS voyage_node;\n",
    "DROP TABLE IF EXISTS voyage_main;\n",
    "\"\"\"\n",
    "\n",
    "DDL_CREATE = \"\"\"\n",
    "CREATE EXTENSION IF NOT EXISTS postgis;\n",
    "\n",
    "CREATE TABLE voyage_main (\n",
    "  id                 SERIAL PRIMARY KEY,\n",
    "  mmsi               BIGINT       NOT NULL,\n",
    "  imo                DOUBLE PRECISION,\n",
    "  call_sign          TEXT,\n",
    "  name               TEXT,\n",
    "  type               DOUBLE PRECISION,\n",
    "  width              DOUBLE PRECISION,\n",
    "  length             DOUBLE PRECISION,\n",
    "  flag               TEXT,\n",
    "  destination        TEXT,\n",
    "  start_time         TIMESTAMP,\n",
    "  end_time           TIMESTAMP,\n",
    "  start_lat          DOUBLE PRECISION,\n",
    "  start_lon          DOUBLE PRECISION,\n",
    "  end_lat            DOUBLE PRECISION,\n",
    "  end_lon            DOUBLE PRECISION,\n",
    "  path_len           INTEGER,\n",
    "  start_port_lat     DOUBLE PRECISION,\n",
    "  start_port_lon     DOUBLE PRECISION,\n",
    "  start_port_name    TEXT,\n",
    "  start_port_country TEXT,\n",
    "  start_port_wpi     INTEGER,\n",
    "  end_port_lat       DOUBLE PRECISION,\n",
    "  end_port_lon       DOUBLE PRECISION,\n",
    "  end_port_name      TEXT,\n",
    "  end_port_country   TEXT,\n",
    "  end_port_wpi       INTEGER,\n",
    "  train              BOOLEAN NOT NULL DEFAULT TRUE\n",
    ");\n",
    "\n",
    "CREATE TABLE voyage_node (\n",
    "  id            SERIAL PRIMARY KEY,\n",
    "  voyage_id     INTEGER NOT NULL REFERENCES voyage_main(id) ON DELETE CASCADE,\n",
    "  timestamp     TIMESTAMP,\n",
    "  latitude      DOUBLE PRECISION,\n",
    "  longitude     DOUBLE PRECISION,\n",
    "  geom          GEOMETRY(Point,4326),\n",
    "  speed         DOUBLE PRECISION,\n",
    "  heading       DOUBLE PRECISION,\n",
    "  course        DOUBLE PRECISION,\n",
    "  draught       DOUBLE PRECISION,\n",
    "  rot           DOUBLE PRECISION,\n",
    "  eta           TIMESTAMP WITH TIME ZONE,\n",
    "  status        DOUBLE PRECISION,\n",
    "  train         BOOLEAN NOT NULL DEFAULT TRUE\n",
    ");\n",
    "\n",
    "CREATE INDEX idx_voyage_node_geom ON voyage_node USING GIST (geom);\n",
    "\"\"\"\n",
    "\n",
    "SQL_INSERT_MAIN = \"\"\"\n",
    "INSERT INTO voyage_main (\n",
    "  mmsi, imo, call_sign, name, type, width, length, flag, destination,\n",
    "  start_time, end_time, start_lat, start_lon, end_lat, end_lon,\n",
    "  path_len, start_port_lat, start_port_lon, start_port_name,\n",
    "  start_port_country, start_port_wpi, end_port_lat, end_port_lon,\n",
    "  end_port_name, end_port_country, end_port_wpi, train\n",
    ") VALUES %s\n",
    "RETURNING id, train;\n",
    "\"\"\"\n",
    "\n",
    "SQL_INSERT_NODE = \"\"\"\n",
    "INSERT INTO voyage_node (\n",
    "  voyage_id, timestamp, latitude, longitude,\n",
    "  speed, heading, course, draught, rot, eta, status, train\n",
    ") VALUES %s;\n",
    "\"\"\"\n",
    "\n",
    "SQL_UPDATE_GEOM = \"\"\"\n",
    "UPDATE voyage_node\n",
    "   SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)\n",
    " WHERE geom IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "# ========== 辅助函数 ==========\n",
    "\n",
    "def list_jsonl_files(data_dir):\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith('.jsonl')]\n",
    "    files.sort(key=lambda f: int(f.split('_')[0]))\n",
    "    return [os.path.join(data_dir, f) for f in files]\n",
    "\n",
    "# ========== 主流程 ==========\n",
    "\n",
    "def import_and_split_fresh():\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    cur  = conn.cursor()\n",
    "\n",
    "    # 1) DROP + CREATE\n",
    "    cur.execute(DDL_DROP)\n",
    "    cur.execute(DDL_CREATE)\n",
    "    conn.commit()\n",
    "\n",
    "    # 2) 遍历并导入\n",
    "    files = list_jsonl_files(DATA_DIR)\n",
    "    for fp in tqdm(files, desc=\"Importing\"):\n",
    "        with open(fp, 'r', encoding='utf-8') as f:\n",
    "            voyages = json.load(f)\n",
    "\n",
    "        # a) 准备 voyage_main 数据，附带 train 列\n",
    "        main_rows = []\n",
    "        for v in voyages:\n",
    "            is_train = random.random() > VALID_RATIO\n",
    "            main_rows.append((\n",
    "                v['MMSI'], v.get('IMO'), v.get('Call Sign'), v.get('Name'),\n",
    "                v.get('Type'), v.get('Width'), v.get('Length'), v.get('Flag'),\n",
    "                v.get('Destination'),\n",
    "                v.get('Start Time'), v.get('End Time'),\n",
    "                v.get('Start Lat'), v.get('Start Lon'),\n",
    "                v.get('End Lat'), v.get('End Lon'),\n",
    "                v.get('Path Len'),\n",
    "                v['Start Port']['Latitude'], v['Start Port']['Longitude'],\n",
    "                v['Start Port']['Main Port Name'], v['Start Port']['Country Code'],\n",
    "                v['Start Port']['World Port Index Number'],\n",
    "                v['End Port']['Latitude'], v['End Port']['Longitude'],\n",
    "                v['End Port']['Main Port Name'], v['End Port']['Country Code'],\n",
    "                v['End Port']['World Port Index Number'],\n",
    "                is_train\n",
    "            ))\n",
    "\n",
    "        # b) 插入主表并收集 (id,train)\n",
    "        id_train_list = []\n",
    "        for i in range(0, len(main_rows), MAIN_BATCH):\n",
    "            batch = main_rows[i:i+MAIN_BATCH]\n",
    "            execute_values(cur, SQL_INSERT_MAIN, batch)\n",
    "            id_train_list += cur.fetchall()\n",
    "        conn.commit()\n",
    "\n",
    "        # c) 准备并插入 voyage_node\n",
    "        node_rows = []\n",
    "        for (vid, vid_train), v in zip(id_train_list, voyages):\n",
    "            for pt in v.get('Path', []):\n",
    "                node_rows.append((\n",
    "                    vid,\n",
    "                    pt.get('timestamp'),\n",
    "                    pt.get('latitude'),\n",
    "                    pt.get('longitude'),\n",
    "                    pt.get('speed'),\n",
    "                    pt.get('heading'),\n",
    "                    pt.get('course'),\n",
    "                    pt.get('draught'),\n",
    "                    pt.get('rot'),\n",
    "                    pt.get('eta'),\n",
    "                    pt.get('status'),\n",
    "                    vid_train\n",
    "                ))\n",
    "        for j in range(0, len(node_rows), NODE_BATCH):\n",
    "            batch = node_rows[j:j+NODE_BATCH]\n",
    "            execute_values(cur, SQL_INSERT_NODE, batch)\n",
    "        conn.commit()\n",
    "\n",
    "        # d) 更新空间字段\n",
    "        cur.execute(SQL_UPDATE_GEOM)\n",
    "        conn.commit()\n",
    "\n",
    "        tqdm.write(f\"{os.path.basename(fp)} imported: \"\n",
    "                   f\"{len(main_rows)} voyages, {len(node_rows)} nodes\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"✅ All done: fresh tables created, data imported, train/valid split applied.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import_and_split_fresh()\n"
   ],
   "id": "73fb327be683acaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SQL=\"\"\"CREATE INDEX idx_voyage_node_vid_ts\n",
    "    ON voyage_node (voyage_id, timestamp);\n",
    "\n",
    "CLUSTER voyage_node USING idx_voyage_node_vid_ts;\"\"\"\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "cur  = conn.cursor()\n",
    "\n",
    "    # 1) DROP + CREATE\n",
    "cur.execute(SQL)\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ],
   "id": "25503eb5ade8d305"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
